{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification on a small set of images using lst format on AWS Sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The dataset is small. It contains the following 20 categories:\n",
    "\n",
    "- bike\n",
    "- crab\n",
    "- ipod\n",
    "- license-plate\n",
    "- owl\n",
    "- playing-card\n",
    "- raccoon\n",
    "- smokestack\n",
    "- spaghetti\n",
    "- syringe\n",
    "- chandelier\n",
    "- grapes\n",
    "- ketch\n",
    "- octopus\n",
    "- paperclip\n",
    "- pyramid\n",
    "- skateboard\n",
    "- soda-can\n",
    "- speed-boat\n",
    "- umbrella\n",
    "\n",
    "The training data contains the above-mentioned 20 categories, with 40 images in each category (800 images total). The testing data includes 40 unclassified images\n",
    "\n",
    "\n",
    "### Get the dataset\n",
    "\n",
    "The dataset is located in an S3 bucket. These images are made publicly accessible to you. For example, one of the images could be viewed via https://bhargav-image-classification.s3.amazonaws.com/20/bike/bike0001.jpg.\n",
    "\n",
    "Each image can be downloaded from a URL, with the following naming convention:\n",
    "\n",
    "**training data**\n",
    "```\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/<category>/<category><index>.jpg\n",
    "\n",
    "for example,\n",
    "\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/octopus/octopus0001.jpg\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/octopus/octopus0002.jpg\n",
    "...\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/octopus/octopus0010.jpg\n",
    "...\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/umbrella/umbrella0040.jpg\n",
    "```\n",
    "\n",
    "**testing data**\n",
    "```\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/<index>.jpg\n",
    "\n",
    "for example,\n",
    "\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/1.jpg\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/2.jpg\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/3.jpg\n",
    "...\n",
    "https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/40.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks - Image Classification\n",
    "Use the provided training data, create a model and classify the unlabeled testing data. In specific, you are exepected to perform the following tasks:\n",
    "\n",
    "1. Download the above-mentioned dataset.\n",
    "2. Organize and prepare training data for training.\n",
    "3. Upload the training data to S3 with the appropriate folder structure.\n",
    "4. Train a model with the training data. \n",
    "5. Perform predictions for the testing data. Output the results including label and accuracy.\n",
    "\n",
    "ToDo:\n",
    "6. Output the result labels and accuracy into a CSV file.  Use this CSV file to either run a query on Athena or Quicksight.\n",
    "7. Tune Hyperparameters for better performance.\n",
    "8. Train a new model with the improved Hyperparameters. \n",
    "9. Perform predictions for the testing data (with the improved model). Output the results including label and accuracy.\n",
    "10. Output the results in step 5 and step 9 into CSV files. Perform queries against the CSV using Athena and / or  perform visualization for the CSV using QuickSight showing differences in accuracy due to tuning hyperparamaters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the dataset. \n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket='veera-learning-series' # customize to your bucket\n",
    "\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'image-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "category = ['bike', 'crab', 'ipod', 'license-plate', 'owl', 'playing-card', 'raccoon', 'smokestack', 'spaghetti', 'syringe', 'chandelier', 'grapes', 'ketch', 'octopus', 'paperclip', 'pyramid', 'skateboard', 'soda-can', 'speed-boat', 'umbrella']\n",
    "\n",
    "for cat in category:\n",
    "    for i in range(1,41):\n",
    "        index = str(i).zfill(4)\n",
    "        download('https://s3-us-west-2.amazonaws.com/lnh-challenge/dataset/ic/'+cat+'/'+cat+index+'.jpg')\n",
    "\n",
    "# Tool for creating lst file\n",
    "download('https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Organize/Prepare the data for SageMaker training. \n",
    "# Hint. You can train with image lst format as what we have done on Friday.\n",
    "\n",
    "# TODO\n",
    "import os\n",
    "category = ['bike', 'crab', 'ipod', 'license-plate', 'owl', 'playing-card', 'raccoon', 'smokestack', 'spaghetti', 'syringe', 'chandelier', 'grapes', 'ketch', 'octopus', 'paperclip', 'pyramid', 'skateboard', 'soda-can', 'speed-boat', 'umbrella']\n",
    "i=1\n",
    "for cat in category:\n",
    "    index = str(i).zfill(3)\n",
    "    di = './inputData/'+index+'.'+cat\n",
    "    print ('mkdir '+di)\n",
    "    os.system('mkdir '+di)\n",
    "    print ('mv '+cat+'* '+di)\n",
    "    os.system('mv '+cat+'* '+di)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p data_train_60\n",
    "for i in inputData/*; do\n",
    "    c=`basename $i`\n",
    "    mkdir -p data_train_60/$c\n",
    "    for j in `ls $i/*.jpg | shuf | head -n 25`; do\n",
    "        mv $j data_train_60/$c/\n",
    "    done\n",
    "done\n",
    "\n",
    "python im2rec.py --list --recursive data-60-train data_train_60/\n",
    "python im2rec.py --list --recursive data-60-val data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 ./data-60-train.lst > example.lst\n",
    "f = open('example.lst','r')\n",
    "lst_content = f.read()\n",
    "print(lst_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Upload to S3 with the correct folder structure. \n",
    "bucket='veera-learning-series'\n",
    "# Four channels: train, validation, train_lst, and validation_lst\n",
    "s3train = 's3://{}/image-classification/train/'.format(bucket)\n",
    "s3validation = 's3://{}/image-classification/validation/'.format(bucket)\n",
    "s3train_lst = 's3://{}/image-classification/train_lst/'.format(bucket)\n",
    "s3validation_lst = 's3://{}/image-classification/validation_lst/'.format(bucket)\n",
    "\n",
    "# upload the image files to train and validation channels\n",
    "!aws s3 cp data_train_60 $s3train --recursive --quiet\n",
    "!aws s3 cp data $s3validation --recursive --quiet\n",
    "\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "!aws s3 cp data-60-train.lst $s3train_lst --quiet\n",
    "!aws s3 cp data-60-val.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Hyperparemeters for the training job.\n",
    "\n",
    "# The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\n",
    "# For this training, we will use 18 layers\n",
    "num_layers = 18\n",
    "# we need to specify the input image shape for the training data\n",
    "image_shape = \"3,224,224\"\n",
    "# we also need to specify the number of training samples in the training set\n",
    "num_training_samples = 500\n",
    "# specify the number of output classes\n",
    "num_classes = 20\n",
    "# batch size for training\n",
    "mini_batch_size = 128\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "# report top_5 accuracy\n",
    "top_k = 5\n",
    "# resize image before training\n",
    "resize = 256\n",
    "# period to store model parameters (in number of epochs), in this case, we will save parameters from epoch 2, 4, and 6\n",
    "checkpoint_frequency = 2\n",
    "# Since we are using transfer learning, we set use_pretrained_model to 1 so that weights can be \n",
    "# initialized with pre-trained weights\n",
    "use_pretrained_model = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Start training job. \n",
    "\n",
    "# First create the training job configuration\n",
    "\n",
    "\n",
    "import time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# create unique job name \n",
    "job_name_prefix = 'sagemaker-imageclassification-notebook'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "training_params = \\\n",
    "{\n",
    "    # specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"image_shape\": image_shape,\n",
    "        \"num_layers\": str(num_layers),\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"mini_batch_size\": str(mini_batch_size),\n",
    "        \"epochs\": str(epochs),\n",
    "        \"learning_rate\": str(learning_rate),\n",
    "        \"top_k\": str(top_k),\n",
    "        \"resize\": str(resize),\n",
    "        \"checkpoint_frequency\": str(checkpoint_frequency),\n",
    "        \"use_pretrained_model\": str(use_pretrained_model)    \n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 360000\n",
    "    },\n",
    "#Training data should be inside a subdirectory called \"train\"\n",
    "#Validation data should be inside a subdirectory called \"validation\"\n",
    "#The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3train,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3validation,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"train_lst\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3train_lst,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation_lst\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3validation_lst,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the configuration, start the training job\n",
    "\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the status of the training job to make sure that it has completed.\n",
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "print (training_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name=\"image-classification-model\" + timestamp\n",
    "print(model_name)\n",
    "info = sage.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n",
    "\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sage.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Endpoint Config\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-epc-' + timestamp\n",
    "endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.p2.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and Deploy Endpoint\n",
    "\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "    \n",
    "try:\n",
    "    sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "\n",
    "    if status != 'InService':\n",
    "        message = sagemaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform predictions for the testing data. \n",
    "# Output the results including label and accuracy.  \n",
    "import boto3\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloand a test image and display it.\n",
    "\n",
    "!wget -O /tmp/test.jpg https://bhargav-image-classification.s3.amazonaws.com/20/ic-test/13.jpg\n",
    "file_name = '/tmp/test.jpg'\n",
    "# test image\n",
    "from IPython.display import Image\n",
    "Image(file_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the image for inference\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read()\n",
    "# result will be in json format and convert it to ndarray\n",
    "result = json.loads(result)\n",
    "# the result will output the probabilities for all classes\n",
    "# find the class with maximum probability and print the class index\n",
    "index = np.argmax(result)\n",
    "category = ['bike', 'crab', 'ipod', 'license-plate', 'owl', 'playing-card', 'raccoon', 'smokestack', 'spaghetti', 'syringe', 'chandelier', 'grapes', 'ketch', 'octopus', 'paperclip', 'pyramid', 'skateboard', 'soda-can', 'speed-boat', 'umbrella']\n",
    "\n",
    "object_categories = category\n",
    "print(\"Result: label - \" + object_categories[index] + \", probability - \" + str(result[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Endpoint\n",
    "sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (bonus) Tune Hyperparameters for better performance.\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Traing a new model with the new Hyperparameters. \n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Perform predictions for the testing data with the improved model. \n",
    "# Output the results including label and accuracy. ##########\n",
    "# Hint. Try to do this task with real-time inference option\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Output the results in step 5 and step 8 into CSV files. \n",
    "# Perform queries against the CSV using Athena. \n",
    "# Perform visualization for the CSV using QuickSight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "\n",
    "It is very important that you clean up the AWS resources you are using after this exercise. This includes the endpoints, the notebook instance, and the data stored in your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
